{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39cc9af4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parametric PINNs for Navier-Stokes\n",
        "# Aditya Jangir ( IIT Delhi )\n",
        "# This Jupyter Notebook contains code for physics-informed neural networks (PINNs). \n",
        "# We refer it as PurePINNs as it does not need any CFD data in the trainning process.\n",
        "# It solves the incompressible Navier–Stokes equations for a wide range of Reynolds numbers. \n",
        "# Instead of training the model for one specific flow case, \n",
        "# the network is trained to learn a general solution that works across many flow conditions. \n",
        "# The results generated using this code are reported in the paper\n",
        "# “A Parameterized Physics-Informed Neural Network Solver for the Navier–Stokes Equations Across Reynolds Numbers”,\n",
        "# Which is available on arXiv: arXiv:2602.04670 and is being prepared for submission to Physics of Fluids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abcbefb3",
      "metadata": {
        "id": "abcbefb3"
      },
      "outputs": [],
      "source": [
        "# import relevent libraries and packages\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import scipy.optimize as spo\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "343979c3",
      "metadata": {
        "id": "343979c3"
      },
      "outputs": [],
      "source": [
        "# Problem / numerical params\n",
        "tf.random.set_seed(42); np.random.seed(42)\n",
        "Re_min, Re_max = 5e1, 3e2  # Range of the Reynolds number for which model is going to train\n",
        "x_min, x_max = 0.0, 1.0    # Define the domain\n",
        "y_min, y_max = 0.0, 1.0\n",
        "D_default = 10             # Hidden layers\n",
        "N_default = 80             # Neurons per hidden layers\n",
        "Nx_cells = 100             # Collocation poins = (Nx_cells)*(Ny_cells)\n",
        "Ny_cells = 100\n",
        "N_b_per_side = 1000        # Boundary points = 4*N_b_per_side\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0af174f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure GPU usage \n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(\"GPU is available and will be used.\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU available. Running on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0ff526",
      "metadata": {
        "id": "0f0ff526"
      },
      "outputs": [],
      "source": [
        "# Neural Network Architecture for Pure PINNs\n",
        "\n",
        "class PurePINN(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 num_hidden_layers=D_default,\n",
        "                 num_neurons_per_layer=N_default,\n",
        "                 include_pressure=True,\n",
        "                 activation='tanh',\n",
        "                 kernel_regularizer=tf.keras.regularizers.L2(1e-8),\n",
        "                 x_mean=0.5, x_scale=0.5,\n",
        "                 y_mean=0.5, y_scale=0.5,\n",
        "                 re_mean=np.log(500.0), re_scale=1.0,\n",
        "                 use_log_re=True,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_neurons_per_layer = num_neurons_per_layer\n",
        "        self.include_pressure = include_pressure\n",
        "        self.use_log_re = use_log_re\n",
        "\n",
        "        # Normalization constants\n",
        "        self.x_mean = tf.constant(x_mean, dtype=tf.float32)\n",
        "        self.x_scale = tf.constant(x_scale, dtype=tf.float32)\n",
        "        self.y_mean = tf.constant(y_mean, dtype=tf.float32)\n",
        "        self.y_scale = tf.constant(y_scale, dtype=tf.float32)\n",
        "        self.re_mean = tf.constant(re_mean, dtype=tf.float32)\n",
        "        self.re_scale = tf.constant(re_scale, dtype=tf.float32)\n",
        "\n",
        "        # Input normalization layer\n",
        "        self.input_norm = tf.keras.layers.Lambda(self._normalize_inputs, name='input_norm')\n",
        "\n",
        "        # Hidden layers\n",
        "        hidden = []\n",
        "        for i in range(num_hidden_layers):\n",
        "            hidden.append(tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "                                                activation=activation,\n",
        "                                                kernel_initializer='glorot_normal',\n",
        "                                                kernel_regularizer=kernel_regularizer,\n",
        "                                                name=f'dense_{i}'))\n",
        "            hidden.append(tf.keras.layers.LayerNormalization(name=f'ln_{i}'))\n",
        "        self.hidden_seq = tf.keras.Sequential(hidden, name='hidden_seq')\n",
        "\n",
        "        # Output layers\n",
        "        self.u_out = tf.keras.layers.Dense(1, activation=None, name='u_out')\n",
        "        self.v_out = tf.keras.layers.Dense(1, activation=None, name='v_out')\n",
        "        if self.include_pressure:\n",
        "            self.p_out = tf.keras.layers.Dense(1, activation=None, name='p_out')\n",
        "        else:\n",
        "            self.p_out = None\n",
        "\n",
        "    def _normalize_inputs(self, X):\n",
        "        x = X[:, 0:1]\n",
        "        y = X[:, 1:2]\n",
        "        re = X[:, 2:3]\n",
        "\n",
        "        x_n = (x - self.x_mean) / (self.x_scale + 1e-8)\n",
        "        y_n = (y - self.y_mean) / (self.y_scale + 1e-8)\n",
        "        if self.use_log_re:\n",
        "            re_n = (tf.math.log(tf.maximum(re, 1e-8)) - self.re_mean) / (self.re_scale + 1e-8)\n",
        "        else:\n",
        "            re_n = (re - tf.exp(self.re_mean)) / (self.re_scale + 1e-8)\n",
        "\n",
        "        return tf.concat([x_n, y_n, re_n], axis=1)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        xn = self.input_norm(inputs)\n",
        "        h = self.hidden_seq(xn, training=training)\n",
        "        u = self.u_out(h)\n",
        "        v = self.v_out(h)\n",
        "        if self.include_pressure:\n",
        "            p = self.p_out(h)\n",
        "        else:\n",
        "            p = tf.zeros_like(u)\n",
        "        return u, v, p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caeea633",
      "metadata": {
        "id": "caeea633"
      },
      "outputs": [],
      "source": [
        "# Domain sampling for lid-driven cavity with variable Reynolds number\n",
        "\n",
        "# Collocation points samplig\n",
        "\n",
        "def stratified_interior_samples(nx_cells, ny_cells, x0=x_min, x1=x_max, y0=y_min, y1=y_max):\n",
        "    xs, ys = [], []\n",
        "    dx = (x1 - x0) / nx_cells\n",
        "    dy = (y1 - y0) / ny_cells\n",
        "    for i in range(nx_cells):\n",
        "        for j in range(ny_cells):\n",
        "            xs.append(x0 + i * dx + np.random.rand() * dx)\n",
        "            ys.append(y0 + j * dy + np.random.rand() * dy)\n",
        "    xs = np.array(xs).reshape(-1, 1)\n",
        "    ys = np.array(ys).reshape(-1, 1)\n",
        "    return xs, ys\n",
        "\n",
        "# Boundary points sampling\n",
        "\n",
        "def boundary_samples(n_per_side):\n",
        "    # Left wall (x=0, y ∈ [0, 1])\n",
        "    y_left = np.random.rand(n_per_side, 1) * (y_max - y_min) + y_min\n",
        "    x_left = np.ones_like(y_left) * x_min\n",
        "    left = np.hstack([x_left, y_left, np.zeros_like(y_left)])  # placeholder Re column\n",
        "\n",
        "    # Right wall (x=1, y ∈ [0, 1])\n",
        "    y_right = np.random.rand(n_per_side, 1) * (y_max - y_min) + y_min\n",
        "    x_right = np.ones_like(y_right) * x_max\n",
        "    right = np.hstack([x_right, y_right, np.zeros_like(y_right)])  # placeholder Re column\n",
        "\n",
        "    # Bottom wall (y=0, x ∈ [0, 1])\n",
        "    x_bot = np.random.rand(n_per_side, 1) * (x_max - x_min) + x_min\n",
        "    y_bot = np.ones_like(x_bot) * y_min\n",
        "    bottom = np.hstack([x_bot, y_bot, np.zeros_like(x_bot)])  # placeholder Re column\n",
        "\n",
        "    # Top lid (y=1, x ∈ [0, 1])\n",
        "    x_top = np.random.rand(n_per_side, 1) * (x_max - x_min) + x_min\n",
        "    y_top = np.ones_like(x_top) * y_max\n",
        "    top = np.hstack([x_top, y_top, np.zeros_like(x_top)])  # placeholder Re column\n",
        "\n",
        "    return left, right, top, bottom\n",
        "\n",
        "# Reynodls number sampling in the defined range\n",
        "\n",
        "def sample_Re(n, log_uniform=True):\n",
        "    if log_uniform:\n",
        "        logmin = np.log(Re_min)\n",
        "        logmax = np.log(Re_max)\n",
        "        return np.exp(np.random.rand(n) * (logmax - logmin) + logmin).reshape(-1, 1)\n",
        "    else:\n",
        "        return (np.random.rand(n, 1) * (Re_max - Re_min) + Re_min)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824a4266",
      "metadata": {
        "id": "824a4266"
      },
      "outputs": [],
      "source": [
        "# PDE residuals definition\n",
        "\n",
        "def pde_residuals(model, X_collocation, Re_vals):\n",
        "    \"\"\"\n",
        "    Compute the PDE residuals for lid-driven cavity (Re-based)\n",
        "    X_collocation: shape (N, 2)  -> [x, y]\n",
        "    Re_vals: shape (N, 1)       -> Reynolds numbers\n",
        "    \"\"\"\n",
        "    # Concatenate to get full input [x, y, Re]\n",
        "    X_input = tf.concat([X_collocation, Re_vals], axis=1)  # shape (N,3)\n",
        "\n",
        "    # Watch input for automatic differentiation\n",
        "    X_input = tf.cast(X_input, tf.float32)\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape2:\n",
        "        tape2.watch(X_input)\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(X_input)\n",
        "            u, v, p = model(X_input, training=True)\n",
        "\n",
        "        # First derivatives\n",
        "        u_x = tape.gradient(u, X_input)[:, 0:1]\n",
        "        u_y = tape.gradient(u, X_input)[:, 1:2]\n",
        "        v_x = tape.gradient(v, X_input)[:, 0:1]\n",
        "        v_y = tape.gradient(v, X_input)[:, 1:2]\n",
        "        p_x = tape.gradient(p, X_input)[:, 0:1]\n",
        "        p_y = tape.gradient(p, X_input)[:, 1:2]\n",
        "\n",
        "    # Second derivatives for diffusion\n",
        "    u_xx = tape2.gradient(u_x, X_input)[:, 0:1]\n",
        "    u_yy = tape2.gradient(u_y, X_input)[:, 1:2]\n",
        "    v_xx = tape2.gradient(v_x, X_input)[:, 0:1]\n",
        "    v_yy = tape2.gradient(v_y, X_input)[:, 1:2]\n",
        "\n",
        "    # Continuity equation residual\n",
        "    r1 = u_x + v_y\n",
        "\n",
        "    # Momentum equation residuals with Re scaling\n",
        "    inv_Re = 1.0 / (Re_vals + 1e-12)\n",
        "    r2 = u * u_x + v * u_y + p_x - inv_Re * (u_xx + u_yy)\n",
        "    r3 = u * v_x + v * v_y + p_y - inv_Re * (v_xx + v_yy)\n",
        "\n",
        "    del tape, tape2\n",
        "    return r1, r2, r3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76867a51",
      "metadata": {
        "id": "76867a51"
      },
      "outputs": [],
      "source": [
        "# Boundary loss definition\n",
        "\n",
        "def boundary_losses(model, left, right, top, bottom, Re_left, Re_right, Re_top, Re_bottom, Ulid=1.0):\n",
        "    # Combine boundary coordinates with Reynolds number as input [x, y, Re]\n",
        "    left_in = tf.concat([left[:, 0:1], left[:, 1:2], Re_left], axis=1)\n",
        "    right_in = tf.concat([right[:, 0:1], right[:, 1:2], Re_right], axis=1)\n",
        "    top_in = tf.concat([top[:, 0:1], top[:, 1:2], Re_top], axis=1)\n",
        "    bottom_in = tf.concat([bottom[:, 0:1], bottom[:, 1:2], Re_bottom], axis=1)\n",
        "\n",
        "    # Predict u, v, p at boundary points (no theta in lid-driven flow)\n",
        "    uL, vL, pL = model(left_in, training=True)\n",
        "    uR, vR, pR = model(right_in, training=True)\n",
        "    uT, vT, pT = model(top_in, training=True)\n",
        "    uB, vB, pB = model(bottom_in, training=True)\n",
        "\n",
        "    # Velocity boundary condition losses\n",
        "    bcV_left = tf.reduce_mean(tf.square(uL) + tf.square(vL))              # Left wall: u=0, v=0\n",
        "    bcV_right = tf.reduce_mean(tf.square(uR) + tf.square(vR))             # Right wall: u=0, v=0\n",
        "    bcV_bottom = tf.reduce_mean(tf.square(uB) + tf.square(vB))            # Bottom wall: u=0, v=0\n",
        "    bcV_top = tf.reduce_mean(tf.square(uT - Ulid) + tf.square(vT))        # Top lid: u=Ulid, v=0\n",
        "\n",
        "    # Total velocity boundary loss\n",
        "    L_bc_V = bcV_left + bcV_right + bcV_bottom + bcV_top\n",
        "\n",
        "    return L_bc_V\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ef81dd",
      "metadata": {
        "id": "82ef81dd"
      },
      "outputs": [],
      "source": [
        "# Training function definition\n",
        "\n",
        "@tf.function\n",
        "def training_step(model, optimizer, X_interior, Re_interior,\n",
        "                  left_xy, right_xy, top_xy, bottom_xy,\n",
        "                  Re_left, Re_right, Re_top, Re_bottom,\n",
        "                  Ulid=1.0):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Compute PDE residuals\n",
        "        r1, r2, r3 = pde_residuals(model, X_interior, Re_interior)\n",
        "        Le1 = tf.reduce_mean(tf.square(r1))  # Continuity\n",
        "        Le2 = tf.reduce_mean(tf.square(r2))  # x-momentum\n",
        "        Le3 = tf.reduce_mean(tf.square(r3))  # y-momentum\n",
        "\n",
        "        # Boundary loss (velocity only)\n",
        "        Lbv = boundary_losses(\n",
        "            model, left_xy, right_xy, top_xy, bottom_xy,\n",
        "            Re_left, Re_right, Re_top, Re_bottom, Ulid\n",
        "        )\n",
        "\n",
        "        # Total loss\n",
        "        Loss_total = Le1 + Le2 + Le3 + Lbv\n",
        "\n",
        "    # Compute gradients\n",
        "    grads = tape.gradient(Loss_total, model.trainable_variables)\n",
        "    max_norm = 5.0\n",
        "    clipped_grads, _ = tf.clip_by_global_norm(grads, max_norm)\n",
        "    # Compute global gradient norm (L2)\n",
        "    grad_norm = tf.linalg.global_norm(grads)\n",
        "\n",
        "    # Apply gradients\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return Loss_total, Le1, Le2, Le3, Lbv, grad_norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09cd2ed1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample interior collocation points (no fixed Re)\n",
        "xs, ys = stratified_interior_samples(Nx_cells, Ny_cells)\n",
        "N_interior = xs.shape[0]\n",
        "\n",
        "# Sample Reynolds numbers for each interior point\n",
        "Re_interior = sample_Re(N_interior, log_uniform=True)\n",
        "\n",
        "# Sample boundary points (each side)\n",
        "left_np, right_np, top_np, bottom_np = boundary_samples(N_b_per_side)\n",
        "\n",
        "# Sample Re for each boundary side\n",
        "Re_left_np   = sample_Re(left_np.shape[0], log_uniform=True)\n",
        "Re_right_np  = sample_Re(right_np.shape[0], log_uniform=True)\n",
        "Re_top_np    = sample_Re(top_np.shape[0], log_uniform=True)\n",
        "Re_bottom_np = sample_Re(bottom_np.shape[0], log_uniform=True)\n",
        "\n",
        "# Convert all data to TensorFlow tensors\n",
        "X_interior_tf = tf.convert_to_tensor(np.hstack([xs, ys]).astype(np.float32))\n",
        "Re_interior_tf = tf.convert_to_tensor(Re_interior.astype(np.float32))\n",
        "\n",
        "left_tf   = tf.convert_to_tensor(left_np[:, 0:2].astype(np.float32))\n",
        "right_tf  = tf.convert_to_tensor(right_np[:, 0:2].astype(np.float32))\n",
        "top_tf    = tf.convert_to_tensor(top_np[:, 0:2].astype(np.float32))\n",
        "bottom_tf = tf.convert_to_tensor(bottom_np[:, 0:2].astype(np.float32))\n",
        "\n",
        "Re_left_tf   = tf.convert_to_tensor(Re_left_np.astype(np.float32))\n",
        "Re_right_tf  = tf.convert_to_tensor(Re_right_np.astype(np.float32))\n",
        "Re_top_tf    = tf.convert_to_tensor(Re_top_np.astype(np.float32))\n",
        "Re_bottom_tf = tf.convert_to_tensor(Re_bottom_np.astype(np.float32))\n",
        "\n",
        "\n",
        "# Build model (using mean log(Re) for normalization)\n",
        "re_mean_log = np.log(5e2).astype(np.float32)  # Mean Re = 500\n",
        "\n",
        "model = PurePINN(num_hidden_layers=D_default,\n",
        "                       num_neurons_per_layer=N_default,\n",
        "                       include_pressure=True,\n",
        "                       x_mean=0.5, x_scale=0.5,\n",
        "                       y_mean=0.5, y_scale=0.5,\n",
        "                       re_mean=re_mean_log,      \n",
        "                       re_scale=1.0,             \n",
        "                       use_log_re=True)        \n",
        "\n",
        "\n",
        "# Dummy input to initialize model weights\n",
        "_dummy_input = tf.convert_to_tensor(\n",
        "    np.hstack([np.array([[0.5, 0.5]]), np.array([[3e2]])]).astype(np.float32)    # Dummy Re = 300\n",
        ")\n",
        "_ = model(_dummy_input)\n",
        "\n",
        "# Print the status of model and number of trainable parameters\n",
        "print(\"Model built. Trainable params:\", sum([np.prod(v.shape) for v in model.trainable_variables]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f33f131",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Adam optimization schedule\n",
        "initial_lr = 5e-3  # Initial learning rate\n",
        "boundaries = [1000, 5000, 15000, 70000, 120000, 200000, 300000] # Epoch numbers at which learning rates is going to change\n",
        "values = [5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6]       # Learning rates correspoinding to above epoch numbers\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=boundaries,\n",
        "    values=values\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "# Training settings\n",
        "EPOCHS_ADAM = 500000           # Total trainning epoches\n",
        "print_every = 1000             # Print loss values at every 1000  epochs\n",
        "checkpoint_every = 1000        # Save the checkpoints (metadata) at 1000 epochs\n",
        "\n",
        "\n",
        "# History containers\n",
        "loss_history = {\n",
        "    \"total\": [],\n",
        "    \"Le1\": [], \"Le2\": [], \"Le3\": [],\n",
        "    \"Lbv\": []\n",
        "}\n",
        "grad_norm_history = []        # Grad norm history\n",
        "\n",
        "\n",
        "# Epoch counter (for resume)\n",
        "epoch_var = tf.Variable(0, dtype=tf.int64)\n",
        "\n",
        "\n",
        "# Checkpoint setup\n",
        "checkpoint_dir = \"./checkpoints_LIDRePurePINNs\"  # Checkpoint directory\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "ckpt = tf.train.Checkpoint(\n",
        "    epoch=epoch_var,\n",
        "    optimizer=optimizer,\n",
        "    model=model\n",
        ")\n",
        "\n",
        "manager = tf.train.CheckpointManager(\n",
        "    ckpt,\n",
        "    checkpoint_dir,\n",
        "    max_to_keep=3\n",
        ")\n",
        "\n",
        "# Restore if checkpoint exists\n",
        "if manager.latest_checkpoint:\n",
        "    ckpt.restore(manager.latest_checkpoint).expect_partial()\n",
        "    print(f\"\\nRestored from checkpoint: {manager.latest_checkpoint}\")\n",
        "    print(f\"Resuming from epoch {epoch_var.numpy()}\")\n",
        "else:\n",
        "    print(\"\\nStarting training from scratch\")\n",
        "\n",
        "start_epoch = int(epoch_var.numpy())\n",
        "\n",
        "\n",
        "# GPU detection\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "num_gpus = len(gpus)\n",
        "\n",
        "if num_gpus > 0:\n",
        "    gpu_names = [gpu.name for gpu in gpus]\n",
        "    print(f\"\\nDetected {num_gpus} GPU(s):\")\n",
        "    for i, gpu in enumerate(gpus):\n",
        "        print(f\"  GPU {i}: {gpu.name}\")\n",
        "else:\n",
        "    gpu_names = [\"CPU\"]\n",
        "    print(\"\\nNo GPU detected. Training will run on CPU.\")\n",
        "\n",
        "\n",
        "# Training loop\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(start_epoch + 1, EPOCHS_ADAM + 1):\n",
        "\n",
        "    loss_total, Le1, Le2, Le3, Lbv, grad_norm = training_step(\n",
        "        model, optimizer,\n",
        "        X_interior_tf, Re_interior_tf,\n",
        "        left_tf, right_tf, top_tf, bottom_tf,\n",
        "        Re_left_tf, Re_right_tf, Re_top_tf, Re_bottom_tf\n",
        "    )\n",
        "\n",
        "    # Update epoch variable (important for resume)\n",
        "    epoch_var.assign(epoch)\n",
        "\n",
        "    # Logging\n",
        "    loss_history[\"total\"].append(loss_total.numpy())\n",
        "    loss_history[\"Le1\"].append(Le1.numpy())\n",
        "    loss_history[\"Le2\"].append(Le2.numpy())\n",
        "    loss_history[\"Le3\"].append(Le3.numpy())\n",
        "    loss_history[\"Lbv\"].append(Lbv.numpy())\n",
        "    grad_norm_history.append(grad_norm.numpy())\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % print_every == 0 or epoch == 1:\n",
        "        elapsed = time.time() - start\n",
        "        print(\n",
        "            f\"[{epoch}/{EPOCHS_ADAM}] \"\n",
        "            f\"loss={loss_total.numpy():.3e} \"\n",
        "            f\"Le1={Le1.numpy():.3e} \"\n",
        "            f\"Le2={Le2.numpy():.3e} \"\n",
        "            f\"Le3={Le3.numpy():.3e} \"\n",
        "            f\"Lbv={Lbv.numpy():.3e} \"\n",
        "            f\"GradNorm={grad_norm.numpy():.3e} \"\n",
        "            f\"t={elapsed:.1f}s \"\n",
        "            f\"GPUs: {num_gpus} ({', '.join(gpu_names)})\"\n",
        "        )\n",
        "\n",
        "        # Query GPU utilization\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\n",
        "                    \"nvidia-smi\",\n",
        "                    \"--query-gpu=utilization.gpu,memory.used,memory.total\",\n",
        "                    \"--format=csv,noheader,nounits\"\n",
        "                ],\n",
        "                stdout=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "            print(f\"    GPU utilization (% | MB used/total): {result.stdout.strip()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"    Could not query GPU utilization: {e}\")\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "  \n",
        "    # Save checkpoint every 1000 epochs\n",
        "    if epoch % checkpoint_every == 0:\n",
        "        save_path = manager.save()\n",
        "        print(f\"Checkpoint saved at epoch {epoch}: {save_path}\")\n",
        "\n",
        "\n",
        "# Save loss history at the end\n",
        "np.save(os.path.join(checkpoint_dir, \"loss_history.npy\"), loss_history)\n",
        "np.save(os.path.join(checkpoint_dir, \"grad_norm_history.npy\"), grad_norm_history)\n",
        "\n",
        "print(\"\\nTraining complete. Histories saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1c893f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# L-BFGS Refinement Stage\n",
        "vars = model.trainable_variables\n",
        "sizes = [int(tf.size(v)) for v in vars]\n",
        "\n",
        "def pack_weights():\n",
        "    \"\"\"Flatten all model parameters into 1D vector.\"\"\"\n",
        "    return np.concatenate([tf.reshape(v, [-1]).numpy() for v in vars]).astype(np.float64)\n",
        "\n",
        "def unpack_and_assign(vec):\n",
        "    \"\"\"Unpack a 1D numpy vector back into model weights.\"\"\"\n",
        "    pos = 0\n",
        "    for v, s in zip(vars, sizes):\n",
        "        slice_val = vec[pos:pos+s].astype(np.float32)\n",
        "        v.assign(tf.reshape(slice_val, v.shape))\n",
        "        pos += s\n",
        "\n",
        "def loss_and_grad(x0):\n",
        "    \"\"\"Compute loss and gradient for L-BFGS.\"\"\"\n",
        "    unpack_and_assign(x0)\n",
        "    with tf.GradientTape() as tape:\n",
        "        r1, r2, r3 = pde_residuals(model, X_interior_tf, Re_interior_tf)\n",
        "        Le1 = tf.reduce_mean(tf.square(r1))\n",
        "        Le2 = tf.reduce_mean(tf.square(r2))\n",
        "        Le3 = tf.reduce_mean(tf.square(r3))\n",
        "\n",
        "        Lbv = boundary_losses(\n",
        "            model, left_tf, right_tf, top_tf, bottom_tf,\n",
        "            Re_left_tf, Re_right_tf, Re_top_tf, Re_bottom_tf, Ulid=1.0\n",
        "        )\n",
        "\n",
        "        Loss_total = Le1 + Le2 + Le3 + Lbv\n",
        "\n",
        "    grads = tape.gradient(Loss_total, model.trainable_variables)\n",
        "    grads = [tf.zeros_like(v) if g is None else g for g, v in zip(grads, model.trainable_variables)]\n",
        "    grad_flat = tf.concat([tf.reshape(g, [-1]) for g in grads], axis=0)\n",
        "\n",
        "    return float(Loss_total.numpy()), grad_flat.numpy().astype(np.float64)\n",
        "\n",
        "# Run SciPy L-BFGS optimizer\n",
        "x0 = pack_weights()\n",
        "print(\" Starting L-BFGS-B optimization...\")\n",
        "t0 = time.time()\n",
        "\n",
        "result = spo.minimize(\n",
        "    fun=loss_and_grad,\n",
        "    x0=x0,\n",
        "    method='L-BFGS-B',\n",
        "    jac=True,\n",
        "    options={'maxiter': 1000, 'ftol': 1e-9, 'maxcor': 50, 'disp': True}\n",
        ")\n",
        "\n",
        "t1 = time.time()\n",
        "print(f\"\\n L-BFGS finished: success={result.success}, message='{result.message}', time={t1 - t0:.1f}s\")\n",
        "\n",
        "# Assign final optimized weights back to model\n",
        "unpack_and_assign(result.x)\n",
        "print(\" Model weights updated with L-BFGS result.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RZS4ie3Rpc49",
      "metadata": {
        "id": "RZS4ie3Rpc49"
      },
      "outputs": [],
      "source": [
        "# Save trained PINN model (.keras format)\n",
        "\n",
        "model.save(\"LIDRePurePINNs.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa12354",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Plot loss curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.semilogy(loss_history[\"total\"], label=\"Total Loss\", linewidth=1)\n",
        "plt.semilogy(loss_history[\"Le1\"], label=\"Continuity Loss\")\n",
        "plt.semilogy(loss_history[\"Le2\"], label=\"Mx Loss)\")\n",
        "plt.semilogy(loss_history[\"Le3\"], label=\"My Loss\")\n",
        "plt.semilogy(loss_history[\"Lbv\"], label=\"BC loss\")\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss (log scale)\", fontsize=12)\n",
        "plt.title(\"Training Loss Evolution\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save losses to file for later analysis\n",
        "np.savez(\"loss_history.npz\", **loss_history)\n",
        "print(\"Saved loss history to 'loss_history.npz'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312f77a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "Nx, Ny = 100, 100   # grid resolution for plotting\n",
        "x = np.linspace(0, 1, Nx)\n",
        "y = np.linspace(0, 1, Ny)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "\n",
        "# Choose Reynolds number for evaluation (can be fixed or changed)\n",
        "Re_plot = 1e2\n",
        "Re_val = np.full_like(X, Re_plot)\n",
        "\n",
        "# Stack grid points into (x, y, Re) tensor\n",
        "X_input = np.stack([X.flatten(), Y.flatten(), Re_val.flatten()], axis=1)\n",
        "X_tf = tf.convert_to_tensor(X_input, dtype=tf.float32)\n",
        "\n",
        "\n",
        "# Evaluate model outputs (u, v, and p)\n",
        "\n",
        "u_pred, v_pred, p_pred = model(X_tf)\n",
        "u_pred = u_pred.numpy().reshape(Ny, Nx)\n",
        "v_pred = v_pred.numpy().reshape(Ny, Nx)\n",
        "p_pred = p_pred.numpy().reshape(Ny, Nx)\n",
        "\n",
        "# Compute velocity magnitude for visualization\n",
        "vel_mag = np.sqrt(u_pred**2 + v_pred**2)\n",
        "\n",
        "\n",
        "# Velocity magnitude contour plot\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "contour = plt.contourf(X, Y, vel_mag, levels=300, cmap='jet')\n",
        "plt.colorbar(contour, label=r\"Velocity Magnitude $|\\mathbf{u}|$\")\n",
        "plt.xlabel(\"x\", fontsize=12)\n",
        "plt.ylabel(\"y\", fontsize=12)\n",
        "plt.title(f\"Velocity Magnitude Field (Re = {Re_plot:.0e})\", fontsize=14)\n",
        "plt.axis(\"equal\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Velocity u contour plot\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "contour = plt.contourf(X, Y, u_pred, levels=300, cmap='jet')\n",
        "plt.colorbar(contour, label=r\"Velocity u $|\\mathbf{u}|$\")\n",
        "plt.xlabel(\"x\", fontsize=12)\n",
        "plt.ylabel(\"y\", fontsize=12)\n",
        "plt.title(f\"Velocity u Field (Re = {Re_plot:.0e})\", fontsize=14)\n",
        "plt.axis(\"equal\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Velocity v contour plot\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "contour = plt.contourf(X, Y, v_pred, levels=300, cmap='jet')\n",
        "plt.colorbar(contour, label=r\"Velocity v $|\\mathbf{u}|$\")\n",
        "plt.xlabel(\"x\", fontsize=12)\n",
        "plt.ylabel(\"y\", fontsize=12)\n",
        "plt.title(f\"Velocity v Field (Re = {Re_plot:.0e})\", fontsize=14)\n",
        "plt.axis(\"equal\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Pressure contour plot\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "contour = plt.contourf(X, Y, p_pred, levels=300, cmap='jet')\n",
        "plt.colorbar(contour, label=\"Pressure $p$\")\n",
        "plt.xlabel(\"x\", fontsize=12)\n",
        "plt.ylabel(\"y\", fontsize=12)\n",
        "plt.title(f\"Pressure Field (Re = {Re_plot:.0e})\", fontsize=14)\n",
        "plt.axis(\"equal\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
